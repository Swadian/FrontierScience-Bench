{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swadian/FrontierScience-Bench-Evaluating-AI-Research-Capabilities-in-LLMs/blob/main/Verdict_Framework_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ioXbt3uuRw",
        "outputId": "5a28acaa-e882-4731-d8f0-e38a464d38d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting verdict\n",
            "  Downloading verdict-0.2.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datasets (from verdict)\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dill (from verdict)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting eval-type-backport (from verdict)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from verdict) (0.20.3)\n",
            "Collecting instructor==1.7.2 (from verdict)\n",
            "  Downloading instructor-1.7.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting krippendorff (from verdict)\n",
            "  Downloading krippendorff-0.8.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting litellm (from verdict)\n",
            "  Downloading litellm-1.68.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting loguru (from verdict)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from verdict) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from verdict) (2.0.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from verdict) (1.76.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from verdict) (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from verdict) (11.2.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from verdict) (2.11.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from verdict) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from verdict) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from verdict) (1.15.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from verdict) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from verdict) (4.13.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from instructor==1.7.2->verdict) (3.11.15)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor==1.7.2->verdict) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from instructor==1.7.2->verdict) (3.1.6)\n",
            "Collecting jiter<0.9,>=0.6.1 (from instructor==1.7.2->verdict)\n",
            "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor==1.7.2->verdict) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from instructor==1.7.2->verdict) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from instructor==1.7.2->verdict) (9.1.2)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from instructor==1.7.2->verdict) (0.15.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->verdict) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->verdict) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->verdict) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->verdict) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai->verdict) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->verdict) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->verdict) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->verdict) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->verdict) (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->verdict) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->verdict) (18.1.0)\n",
            "Collecting dill (from verdict)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->verdict)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->verdict)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->verdict)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->verdict) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->verdict) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->verdict) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm->verdict) (8.1.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm->verdict) (8.7.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm->verdict) (4.23.0)\n",
            "Collecting openai (from verdict)\n",
            "  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm->verdict)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm->verdict)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->verdict) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->verdict) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->verdict) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->verdict) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->verdict) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.7.2->verdict) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.7.2->verdict) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.7.2->verdict) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.7.2->verdict) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.7.2->verdict) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.7.2->verdict) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.7.2->verdict) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->verdict) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->verdict) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->verdict) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->verdict) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm->verdict) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor==1.7.2->verdict) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->verdict) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->verdict) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->verdict) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->verdict) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->verdict) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor==1.7.2->verdict) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->instructor==1.7.2->verdict) (2.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm->verdict) (2024.11.6)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.9.0->instructor==1.7.2->verdict) (1.5.4)\n",
            "Downloading verdict-0.2.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.7.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading krippendorff-0.8.1-py3-none-any.whl (18 kB)\n",
            "Downloading litellm-1.68.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.75.0-py3-none-any.whl (646 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, python-dotenv, loguru, krippendorff, jiter, fsspec, eval-type-backport, dill, tiktoken, multiprocess, openai, litellm, instructor, datasets, verdict\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.9.0\n",
            "    Uninstalling jiter-0.9.0:\n",
            "      Successfully uninstalled jiter-0.9.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.76.2\n",
            "    Uninstalling openai-1.76.2:\n",
            "      Successfully uninstalled openai-1.76.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 eval-type-backport-0.2.2 fsspec-2025.3.0 instructor-1.7.2 jiter-0.8.2 krippendorff-0.8.1 litellm-1.68.0 loguru-0.7.3 multiprocess-0.70.16 openai-1.75.0 python-dotenv-1.1.0 tiktoken-0.9.0 verdict-0.2.2 xxhash-3.5.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import verdict\n",
        "except:\n",
        "  !pip install verdict\n",
        "  import verdict\n",
        "from verdict import Pipeline, Layer\n",
        "from verdict.common.judge import CategoricalJudgeUnit\n",
        "from verdict.scale import DiscreteScale\n",
        "from verdict.schema import Schema, Field\n",
        "from verdict.common.judge import JudgeUnit\n",
        "from verdict.transform import MapUnit, MaxPoolUnit\n",
        "from collections import Counter\n",
        "from google.colab import drive, userdata\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knWPZ9jsurso"
      },
      "outputs": [],
      "source": [
        "rubric_guidelines = \"\"\"\n",
        "The rubric specifications are as follows:\n",
        "- Each paper will be assigned a score between 1 and 10 (inclusive), based on how similar it is to the original paper. A higher score indicates the predicted methodology is more similar to the original methodology.\n",
        "- When assigning a score to a paper, clearly explain what parts of the paper's methodology are similar or different from the original paper.\n",
        "- Break down your thought process by identifying key strengths, weaknesses, and any alignment or discrepancies with the paper.\n",
        "- The following rubric will be used to assign scores:\n",
        "  - A score of 1 represents a methodology that is vastly different in the approach and execution compared to the original paper.\n",
        "  - A score of 2 represents a methodology that is significantly different but contains a few minor similarities.\n",
        "  - A score of 3 represents a methodology that has some similarities but misses key details in the approach and execution compared to the original paper.\n",
        "  - A score of 4 represents a methodology that is somewhat similar but still lacks important aspects of the original paper.\n",
        "  - A score of 5 represents a methodology that has a relatively equal number of similarites and differences in approach and execution compared to the original paper.\n",
        "  - A score of 6 represents a methodology that is fairly close to the originial but omits or alters some details.\n",
        "  - A score of 7 represents a methodology that closely matches the original in approach and execution but with minor alterations.\n",
        "  - A score of 8 represents a methodology that is very similar to the original with only small, noncritical differences in the execution.\n",
        "  - A score of 9 represents a methodology that is nearly identical to the original paper, with differences that are extremely minor and almost inconsqeuential.\n",
        "  - A score of 10 represents a methodology that is almost identical to the approach and execution of the original paper, with trivial differences ignored.\n",
        "\n",
        "A hypothetical example paper and its key methodological details are provided below. This is PURELY an example reference paper to gauge the differences between each score.\n",
        "Examples of scores of 1-10 (inclusive) are also provided, with a description of the key similarities and differences that justify that score being assigned.\n",
        "\n",
        "The example paper is titled:\n",
        "“Efficient Fine‑Tuning Strategies for Domain‑Specific Language Models in Legal Document Analysis.”\n",
        "\n",
        "Its key methodological details include:\n",
        "Pre‑Trained Model: Uses a pre‑trained BERT base model (12 layers, 110M parameters).\n",
        "Fine‑Tuning Data: Fine‑tuned on a corpus of 100,000 legal documents.\n",
        "Domain‑Adaptive Pre‑Training (DAPT): Begins with a DAPT phase using masked language modeling for 3 epochs.\n",
        "Classification Fine‑Tuning: Followed by fine‑tuning with a classification head for legal issue categorization over 4 epochs.\n",
        "Hyperparameters: Employs a batch size of 32 and a learning rate of 2×10⁻⁵.\n",
        "Evaluation: Performance is measured using Accuracy, F1‑Score, and ROC‑AUC on a held‑out test set of 10,000 documents.\n",
        "Ablation Study: An ablation study is conducted comparing performance with and without the DAPT phase, with statistical significance assessed via paired t‑tests.\n",
        "\n",
        "- Score 1 Example: \"The evaluated paper’s methodology is vastly different from the original.\n",
        "  Instead of leveraging a pre‑trained BERT model, the study builds an LSTM network from scratch without any pre‑training phase.\n",
        "  The dataset comprises only 2,000 blog posts rather than legal documents, and there is no Domain‑Adaptive Pre‑Training (DAPT) phase.\n",
        "  The paper directly trains the model for text classification using arbitrary hyperparameters (e.g., a batch size of 8 and a learning rate of 1×10⁻³)\n",
        "  and evaluates performance solely based on training loss rather than using standard metrics like Accuracy, F1‑Score, or ROC‑AUC.\n",
        "  Additionally, no ablation study is performed. These fundamental differences in model architecture, data, training strategy, hyperparameters,\n",
        "  evaluation metrics, and analysis justify a score of 1.”\n",
        "\n",
        "- Score 2 Example: \"The evaluated paper shares minimal similarities with the original methodology and differs significantly in approach and execution.\n",
        "  While it does use a transformer-based model, it fine-tunes a GPT-2 model instead of a pre-trained BERT base model. The dataset consists of only\n",
        "  5,000 general legal summaries rather than a large corpus of 100,000 legal documents.There is no Domain-Adaptive Pre-Training (DAPT) phase,\n",
        "  and the model is fine-tuned directly for 1 epoch using a batch size of 16 and a learning rate of 1×10⁻³.Evaluation is based solely on perplexity\n",
        "  rather than classification-focused metrics like Accuracy, F1-Score, or ROC-AUC. No ablation study is performed. These fundamental departures\n",
        "  from the original methodology justify a score of 2.\"\n",
        "\n",
        "- Score 3 Example: \"This evaluated methodology shows some elements of an ML approach but deviates significantly from the original.\n",
        "  While it uses a pre‑trained BERT model, the study fine‑tunes on a dataset of 20,000 news articles instead of 100,000 legal documents.\n",
        "  The paper omits the Domain‑Adaptive Pre‑Training phase, moving directly to fine‑tuning for classification with only 2 epochs rather than 4.\n",
        "  It employs a batch size of 64 and a learning rate of 5×10⁻⁵, which diverge notably from the original hyperparameters.\n",
        "  The evaluation relies solely on Accuracy, ignoring F1‑Score and ROC‑AUC, and it does not include any ablation study.\n",
        "  These substantial differences in data selection, training procedure, hyperparameter configuration, and evaluation methods warrant a score of 3.\"\n",
        "\n",
        "- Score 4 Example: \"The evaluated methodology shares some methodological elements with the original but deviates in key aspects. It uses a pre-trained BERT\n",
        "  base model, but the fine-tuning dataset consists of 30,000 legal documents instead of 100,000. The study skips the Domain-Adaptive Pre-Training (DAPT) phase\n",
        "  entirely and instead applies a simple transfer learning approach. The fine-tuning runs for only 3 epochs instead of 4, with a batch size of 40\n",
        "  and a learning rate of 3×10⁻⁵. Evaluation includes Accuracy and F1-Score but omits ROC-AUC, and the ablation study is limited to a single comparison\n",
        "  without statistical significance testing. While the methodology retains some recognizable aspects of the original, the substantial differences in\n",
        "  data scale, training procedure, hyperparameters, and analysis warrant a score of 4.\"\n",
        "\n",
        "- Score 5 Example: \"The evaluated paper’s methodology exhibits a balanced mix of similarities and differences compared to the original.\n",
        "  Like the original, it employs a BERT base model and focuses on a classification task. However, it fine‑tunes on a smaller dataset of 50,000\n",
        "  documents (with only 40,000 being legal texts and 10,000 from related domains), and it foregoes the Domain‑Adaptive Pre‑Training (DAPT) phase\n",
        "  entirely. The fine‑tuning phase runs for 4 epochs as in the original but uses a slightly different batch size of 28 (which is not detrimental to the score)\n",
        "  and a learning rate of 2×10⁻⁵. Evaluation includes Accuracy and F1‑Score but omits ROC‑AUC, and there is a brief, underdeveloped ablation study\n",
        "  comparing two training settings rather than a comprehensive analysis with paired t‑tests. These comparable yet noticeably different choices\n",
        "  justify a score of 5.\n",
        "\n",
        "- Score 6 Example: The evaluated methodology is fairly close to the original but omits or alters some details. It still employs the pre‑trained BERT\n",
        "  base model and targets legal document classification, yet instead of using the full dataset of 100,000 legal documents, it utilizes around 80,000 documents.\n",
        "  Notably, the Domain‑Adaptive Pre‑Training (DAPT) phase is entirely omitted, with the paper proceeding directly to fine‑tuning for classification over 4 epochs.\n",
        "  Additionally, while the original hyperparameters include a batch size of 32 and a learning rate of 2×10⁻⁵, the evaluated study slightly adjusts the learning rate\n",
        "  to 2.2×10⁻⁵, though it maintains the same batch size. The evaluation is conducted using Accuracy and F1‑Score, but ROC‑AUC is not reported, and the ablation study\n",
        "  is simplified, comparing only a basic variant of the methodology rather than performing a full paired t‑test analysis. These omissions and modifications while retaining\n",
        "  the core framework justify a score of 6.\n",
        "\n",
        "- Score 7 Example: \"The evaluated methodology resembles the original in several high‑level components but introduces notable differences in key areas.\n",
        "  Like the original, it utilizes a pre‑trained BERT base model and targets legal document classification. However, it fine‑tunes on a corpus of\n",
        "  120,000 documents, where only 70,000 are strictly legal documents and the remainder are general documents from adjacent domains—altering the\n",
        "  data composition significantly. Instead of a pure DAPT phase with masked language modeling for 3 epochs, the evaluated paper implements a\n",
        "  combined DAPT phase that includes both masked language modeling and next sentence prediction over 3 epochs. Additionally, while the fine‑tuning\n",
        "  phase still runs for 4 epochs, it adopts a dynamic learning rate schedule that starts at 2×10⁻⁵ but decays to 1×10⁻⁵ halfway through training.\n",
        "  For evaluation, the paper reports Accuracy and F1‑Score but omits ROC‑AUC, and the ablation study is conducted using descriptive performance\n",
        "  comparisons rather than paired t‑tests. These differences in dataset composition, training regimen, learning rate strategy, evaluation metrics,\n",
        "  and statistical analysis indicate a good overall alignment in the core methodology while incorporating several meaningful variations, warranting\n",
        "  a score of 7.”\n",
        "\n",
        "- Score 8 Example: \"The evaluated methodology is nearly identical to the original, with only a few minor adjustments. It employs the same pre‑trained BERT base model\n",
        "  and uses a corpus almost identical to the original 100,000 legal documents—except that about 5% of the documents are contracts rather than legal case records.\n",
        "  The Domain‑Adaptive Pre‑Training (DAPT) phase is retained at 3 epochs, with the only change being the addition of a small contrastive learning component alongside masked language modeling.\n",
        "  The fine‑tuning stage mirrors the original by running for 4 epochs with a batch size of 32, though the learning rate is slightly adjusted to 1.8×10⁻⁵ (a modest change from 2×10⁻⁵).\n",
        "  Evaluation continues to use Accuracy, F1‑Score, and ROC‑AUC, and while the ablation study largely follows the original structure, it adds a single extra experiment to analyze an alternative pre‑training strategy.\n",
        "  These slight modifications in data composition, training, and analysis are minor enough to justify a score of 8, as they do not significantly alter the core methodology.\"\n",
        "\n",
        "- Score 9 Example: \"The evaluated methodology is nearly identical to the original, with only extremely minor variations. It employs the same pre-trained BERT base model and fine-tunes on a dataset of 100,000 legal documents,\n",
        "  maintaining the same proportions of legal case records and statutes. The Domain-Adaptive Pre-Training (DAPT) phase follows the same masked language modeling procedure for 3 epochs, and the fine-tuning process\n",
        "  runs for 4 epochs with a batch size of 32. The only slight modification is the learning rate schedule, which begins at 2×10⁻⁵ but decays linearly to 1.5×10⁻⁵ over training.\n",
        "  Evaluation includes Accuracy, F1-Score, and ROC-AUC, and the ablation study mirrors the original but includes one extra comparison on a different random seed.\n",
        "  Since these differences are minimal and do not significantly affect the methodology’s structure, a score of 9 is appropriate.\"\n",
        "\n",
        "- Score 10 Example: \"The evaluated methodology is almost identical to that of the original paper. It employs the same pre‑trained BERT base model and\n",
        "  fine‑tunes on an identical corpus of 100,000 legal documents. The methodology follows the original two‑phase training process: an\n",
        "  initial Domain‑Adaptive Pre‑Training (DAPT) phase using masked language modeling for 3 epochs, followed by a 4‑epoch fine‑tuning phase with a\n",
        "  classification head. The hyperparameters—batch size of 32 and learning rate of 2×10⁻⁵—match exactly. Performance is evaluated using Accuracy,\n",
        "  F1‑Score, and ROC‑AUC on a held‑out test set of 10,000 documents, and the paper includes a comprehensive ablation study comparing the impact of\n",
        "   DAPT, with paired t‑tests verifying statistical significance. Any differences are purely stylistic or due to formatting, thus justifying a score of 10.\"\n",
        "\"\"\"\n",
        "\n",
        "trivial_differences = \"\"\"\n",
        "Below is a list of illustrative examples showing that minor or trivial differences in a predicted methodology should not be penalized when the overarching\n",
        "ideas and experimental approaches remain intact. These examples are drawn from various aspects of AI/ML research papers in general:\n",
        "\n",
        "Alternate Evaluation Metrics with the Same Objective:\n",
        "Original Methodology: Evaluates a classification model using Accuracy and F1‑Score to measure overall performance.\n",
        "Predicted Methodology: Uses Accuracy and Recall (or even macro‑F1) as evaluation metrics.\n",
        "Illustration: Although the specific metrics differ slightly, both methods aim to assess classification performance. The predicted approach still\n",
        "captures the essence of performance evaluation without a substantial departure from the original intent.\n",
        "\n",
        "Slight Variation in Data Preprocessing Techniques:\n",
        "Original Methodology: Applies z‑score normalization to all input features to standardize the data before training.\n",
        "Predicted Methodology: Uses min‑max scaling for data normalization instead.\n",
        "Illustration: Both normalization techniques are standard practices in machine learning. While the scaling method is different, the core goal—ensuring\n",
        "that the features are on a similar scale—is maintained, and thus this variation is trivial.\n",
        "\n",
        "Different Hyperparameter Scheduling with Similar Impact:\n",
        "Original Methodology: Utilizes a fixed learning rate schedule during training.\n",
        "Predicted Methodology: Implements cosine annealing for learning rate decay throughout training.\n",
        "Illustration: Even though the scheduling strategy differs, both approaches aim to optimize convergence during training. The alternative schedule is\n",
        "an acceptable variation as it preserves the overall training philosophy.\n",
        "\n",
        "Minor Architectural Adjustments in Model Design:\n",
        "Original Methodology: Uses a ResNet‑50 architecture with a standard block configuration for image classification.\n",
        "Predicted Methodology: Adopts a slightly modified ResNet‑50 where dropout layers are added after certain convolutional blocks to improve regularization.\n",
        "Illustration: The predicted methodology retains the high‑level structure and intent of using a ResNet‑based architecture. The addition of dropout is a\n",
        "minor tweak aimed at enhancing performance without changing the core design.\n",
        "\n",
        "Alternate Statistical Analysis in Ablation Studies:\n",
        "Original Methodology: Conducts an ablation study with statistical significance determined using paired t‑tests.\n",
        "Predicted Methodology: Uses a non‑parametric test, such as the Wilcoxon signed‑rank test, to assess the significance of differences in performance.\n",
        "Illustration: The change in statistical method does not undermine the overall experimental design. Both tests are rigorous and accepted in the field, so\n",
        "the predicted methodology remains fundamentally aligned with the original goal of validating the results.”\n",
        "\n",
        "Alternate Magnitudes of Values:\n",
        "Original Methodology: Training dataset consisted of 100,000 images\n",
        "Predicted Methodology: Training dataset consisted of 90,000 images\n",
        "Illustration: The change in the size of the dataset is still sufficiently large to be considered representative of the original distribution, ensuring that\n",
        "the model’s performance remains comparable despite the slight reduction in training data.\n",
        "\"\"\"\n",
        "\n",
        "judge_prompt = f\"\"\"You will be given the TRUE research paper contributions then\n",
        "you will be given the PREDICTED research paper contributions. Both are in JSON format.\n",
        "\n",
        "Instructions:\n",
        "- You are an honest and analytical judge and you think through things carefully.\n",
        "- You will compare how similar the PREDICTED contributions are to the TRUE contributions based on a rubric that will be specified below.\n",
        "- Do NOT consider stylistic/writing choices in your comparison.\n",
        "- Do NOT consider trivial details in your comparison.\n",
        "- Prioritize clarity, correctness, and alignment of the ideas with the research problem over the use of mathematical notation. A methodology can still be valid and well-reasoned even without formal math notation.\n",
        "\n",
        "{rubric_guidelines}\n",
        "\n",
        "{trivial_differences}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i4nCdLqoTT6",
        "outputId": "a8432299-a60b-42af-eed8-a59cd7f1fb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading prediction and ground truth data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Processing paper_001 (2/101) ==========\n",
            "[paper_001] Extracted ground truth and prediction.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [00:14<00:14, 14.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[paper_001] Mode scores: {'GPT-4o': 1, 'o3-mini': 2, 'Claude 3.5': 3, 'Grok 2': 3}\n",
            "[paper_001] Mode explanations: {'GPT-4o': '1. Different Approach: The predicted methodology focuses on developing an AI-powered system using CNNs and SVMs for image and text processing, whereas the true methodology focuses on sentiment analysis, web scraping, and association rule mining.\\n\\n2. Lack of Key Modules: The predicted methodology does not mention the specific modules used in the true methodology (Web scrapper, Sentiment analysis, Association rule mining, Data analysis dashboard).\\n\\n3. Different Data Sources: The predicted methodology includes laboratory and field test data, which is not mentioned in the true methodology. The true methodology focuses on Amazon reviews and images.\\n\\n4. Different Techniques: The true methodology uses LSTM for sentiment analysis and FP-growth for association rule mining, whereas the predicted methodology uses CNN and SVM for feature extraction and prediction.\\n\\n5. Different Evaluation and Use: The predicted methodology is more about prediction and user interface for engineers, while the true methodology focuses on identifying packaging failures and understanding customer sentiment through specific techniques.\\n\\nThese substantial differences in approach, techniques, and goals justify a score of 3.', 'o3-mini': 'The predicted methodology and the original approach share the high-level goal of creating an AI‐driven packaging evaluation system. However, their implementations diverge significantly. In the ground truth, the process is split into modular components (web scraper, sentiment analysis, association rule mining, and data analysis) that focus heavily on processing Amazon customer reviews via techniques like LSTM-based sentiment analysis, TF-IDF for keyword extraction, and FP-Growth for association rule mining. In contrast, the predicted methodology introduces a very different pipeline: it includes extensive image preprocessing with convolutional neural networks (CNNs) for feature extraction and uses a support vector machine (SVM) for predicting damage likelihood, while its text processing steps involve more generic NLP techniques rather than the sentiment analysis approaches described in the ground truth. It also incorporates additional data sources (laboratory and field test data) not mentioned in the original. These differences in data sources, model choices (CNN/SVM vs. LSTM/association rule mining), and the overall flow represent major departures from the original methodology. Hence, while the overarching objective remains similar, the approaches are fundamentally different, justifying a low similarity score.', 'Claude 3.5': 'The predicted methodology shows some similarities to the original but misses several key aspects and introduces different approaches:\\n\\nSimilarities:\\n- Both methodologies aim to analyze and evaluate packaging performance using data-driven approaches\\n- Both incorporate elements of data processing and analysis\\n- Both aim to provide insights for packaging improvement\\n\\nKey Differences:\\n1. Core Approach:\\n- Original: Web-based platform (WIPE) focused on sentiment analysis of customer reviews and association rule mining\\n- Predicted: Primarily focuses on image processing and machine learning with CNNs/SVMs\\n\\n2. Data Sources:\\n- Original: Specifically uses Amazon.com reviews with a web scraper to collect text and images\\n- Predicted: Uses a more general approach with multiple data sources including lab tests\\n\\n3. Analysis Methods:\\n- Original: Uses specific techniques like LSTM for sentiment analysis and FP-growth for association rule mining\\n- Predicted: Focuses on CNN-based image analysis and SVM classification\\n\\n4. Implementation:\\n- Original: Details specific modules (Web scraper, Sentiment analysis, Association rule mining, Dashboard)\\n- Predicted: Describes a more general ML pipeline without the specific web-based implementation\\n\\nThe predicted methodology misses key elements like:\\n- The web scraping component\\n- The specific focus on Amazon reviews\\n- The association rule mining approach\\n- The detailed sentiment analysis comparison (LSTM vs. Naive Bayes vs. lexicon-based)\\n\\nWhile there are some overlapping concepts around data analysis and packaging evaluation, the fundamental approach and key methodological components are quite different, warranting a score of 3.', 'Grok 2': 'The predicted methodology is significantly different from the ground truth methodology in terms of approach and execution. While the ground truth methodology focuses on a web-based platform using customer reviews from Amazon, the predicted methodology involves a broader AI-powered system that incorporates multiple data sources, including images, textual feedback, and laboratory/field test data. The ground truth methodology employs specific techniques like LSTM for sentiment analysis and FP-growth for association rule mining, whereas the predicted methodology uses CNNs for image feature extraction and an SVM for model training, which are entirely different approaches. The predicted methodology does not mention web scraping, sentiment analysis, or association rule mining, which are key components of the ground truth methodology. The predicted methodology includes a user-friendly interface for packaging engineers, which is not mentioned in the ground truth methodology. These fundamental departures from the original methodology justify a score of 2.'}\n",
            "[paper_001] Aggregated score: 2.25\n",
            "\n",
            "========== Processing paper_002 (3/101) ==========\n",
            "[paper_002] Extracted ground truth and prediction.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:30<00:00, 15.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[paper_002] Mode scores: {'GPT-4o': 3, 'o3-mini': 3, 'Claude 3.5': 3, 'Grok 2': 3}\n",
            "[paper_002] Mode explanations: {'GPT-4o': \"The predicted methodology diverges significantly from the original paper. The original study uses a specific dataset (X-Fact) and detailed experimental setup focusing on particular language families and specific LLMs like GPT-4, Mistral, and Llama, with a focus on translation techniques and translation bias. The predicted methodology, however, describes a broader and more generic approach to multilingual claim verification without referencing the specific dataset or experimental setup used in the original study. It mentions general dataset creation and selection of multilingual LLMs but does not specify the models used in the original study. Additionally, the predicted methodology includes components like bias analysis, cross-lingual transfer evaluation, calibration analysis, and robustness evaluation, which are not explicitly covered in the original paper. The evaluation metrics mentioned in the predicted methodology are more comprehensive than those in the original study, but this further highlights the difference in scope and focus between the two papers. Overall, the predicted methodology's approach and execution are vastly different from the original paper.\", 'o3-mini': 'The predicted methodology shares some high‐level aspects with the ground truth; both address multilingual claim verification by detailing dataset creation and the selection of multilingual language models with an experimental setup. However, there are significant departures. The truth methodology is very detailed in describing the dataset composition (with 600 claims per language family), the specific multilingual models used (including GPT‑4 variants, Mistral, Llama, and a fine‑tuned mBERT), and uniquely focuses on three translation techniques (Direct Inference, Self‑Translate, and Pre‑Translate) as well as evaluation via correct/incorrect/inconclusive responses and translation bias measured by COMETKIWI. The predicted methodology, on the other hand, adopts a more generic and broader approach. It substitutes the specialized translation techniques with the inclusion of calibration, adversarial robustness, and cross‑lingual transfer evaluation, introduces different evaluation metrics (precision, recall, AUC, etc.), and lists different models (Llama 2, BLOOMZ, GPT‑4) without the specific details given in the ground truth. These differences in key experimental procedures, translation evaluation, and model details justify that while there are some similarities in general structure, major details are missing or altered. Therefore, a score of 3 is appropriate.', 'Claude 3.5': \"The predicted methodology shows some similarities with the original paper but misses several key details and adds many elements not present in the true methodology. Here's the detailed analysis:\\n\\nSimilarities:\\n- Both methodologies involve working with multilingual language models for claim verification\\n- Both include evaluation of model performance across different languages\\n- Both consider high-resource vs low-resource language performance\\n\\nKey Differences:\\n1. Dataset:\\n- Original uses specifically the X-Fact dataset with 600 claims per language family and 5 veracity labels\\n- Predicted is vague about dataset size and structure, mentioning general data curation without specifics\\n\\n2. Models:\\n- Original clearly specifies exact models (GPT-4, GPT-4 Mini, Mistral Large 2, Llama 3.1 variants, mBERT)\\n- Predicted mentions some models (Llama 2, BLOOMZ, GPT-4) but is less specific and includes models not in the original\\n\\n3. Translation Techniques:\\n- Original clearly defines three specific approaches (Direct Inference, Self-Translate, Pre-Translate)\\n- Predicted doesn't mention these crucial translation techniques at all\\n\\n4. Evaluation Metrics:\\n- Original focuses on correct/incorrect/inconclusive responses and accuracy\\n- Predicted adds many metrics not in original (F1-score, precision, recall, AUC, etc.)\\n\\n5. Additional Elements Not in Original:\\n- Predicted adds extensive discussion of fine-tuning, hyperparameter tuning, and adversarial training\\n- Includes explainability techniques and calibration analysis not present in original\\n- Discusses defense mechanisms and robustness evaluation not mentioned in original\\n\\nThe score of 3 is warranted because while the predicted methodology shows some understanding of multilingual LLM evaluation, it misses core elements of the original paper's approach (translation techniques) while adding many elements not present in the original. The differences in dataset specification, model selection, and evaluation metrics represent significant departures from the original methodology.\", 'Grok 2': 'The predicted methodology shares some elements with the ground truth methodology but deviates significantly in several key areas. Both methodologies focus on multilingual claim verification using LLMs, but the predicted methodology includes additional components such as cross-lingual transfer evaluation, calibration analysis, and robustness evaluation, which are not present in the ground truth methodology. The predicted methodology also uses a more diverse set of LLMs and a more comprehensive set of evaluation metrics. However, the predicted methodology lacks specific details about the dataset, such as the number of claims per language family and the distribution of veracity labels, which are crucial for a fair comparison. Additionally, the predicted methodology does not mention the use of instruction-tuned models or the specific translation techniques employed in the ground truth methodology. These substantial differences in data selection, model selection, evaluation metrics, and experimental setup warrant a score of 3.'}\n",
            "[paper_002] Aggregated score: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from verdict.util import ratelimit\n",
        "ratelimit.disable()\n",
        "\n",
        "# =============================================================================\n",
        "# Global Flags for Model Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "# Use Self-Omission Judging : Predictor Model should be set to False\n",
        "EVAL_GPT_4O          = True\n",
        "EVAL_O3_MINI         = True\n",
        "EVAL_CLAUDE_35SONNET = True\n",
        "EVAL_GEMINI_15PRO    = False\n",
        "EVAL_GROK_2          = True\n",
        "\n",
        "# =============================================================================\n",
        "# Setup API Clients and Models\n",
        "# =============================================================================\n",
        "\n",
        "# https://verdict.haizelabs.com/docs/concept/model/#usage\n",
        "# https://docs.litellm.ai/docs/set_keys\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"algoverse-openai\")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"algoverse-anthropic\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"algoverse-gemini\")\n",
        "os.environ[\"XAI_API_KEY\"] = userdata.get(\"grok\")\n",
        "os.environ[\"XAI_BASE_URL\"] = \"https://api.x.ai/v1\"\n",
        "\n",
        "# =============================================================================\n",
        "# Data Loading and Result Storage Setup\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading prediction and ground truth data...\")\n",
        "predictions    = pickle.load(open('/content/drive/MyDrive/ResearchPapers/gemini_official_predictions/predictions.pkl', 'rb'))\n",
        "ground_truths  = pickle.load(open('/content/drive/MyDrive/ResearchPapers/ground_truth.pkl', 'rb'))\n",
        "\n",
        "results_dir = \"/content/drive/MyDrive/ResearchPapers/Verdict_Judging\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions\n",
        "# =============================================================================\n",
        "\n",
        "score_scale = DiscreteScale([i for i in range(1, 11)])\n",
        "\n",
        "def create_repeated_judge_layer(name, model_name):\n",
        "  # https://verdict.haizelabs.com/docs/concept/unit/#unit\n",
        "  # https://colab.research.google.com/github/haizelabs/verdict/blob/main/notebooks/common/judge.ipynb\n",
        "  judge = (\n",
        "    CategoricalJudgeUnit(name=f\"{name} Judge\", categories=score_scale, explanation=True)\n",
        "    .prompt(judge_prompt + \"\"\"\\n\n",
        "    Ground truth methodology:\n",
        "    {source.ground_truth}\n",
        "\n",
        "    Predicted methodology:\n",
        "    {source.predicted}\n",
        "    \"\"\")\n",
        "    .via(model_name, retries=3, temperature=0.7)\n",
        "  )\n",
        "  return Layer(judge, repeat=5) >> MaxPoolUnit() # MapUnit(compute_mode_with_explanation)\n",
        "\n",
        "# Define judge units per model\n",
        "judging_units = []\n",
        "included_judges = []\n",
        "if EVAL_GPT_4O:\n",
        "  judging_units.append(create_repeated_judge_layer(\"GPT-4o\", \"gpt-4o\"))\n",
        "  included_judges.append(\"GPT-4o\")\n",
        "if EVAL_O3_MINI:\n",
        "  judging_units.append(create_repeated_judge_layer(\"o3-mini\", \"o3-mini\"))\n",
        "  included_judges.append(\"o3-mini\")\n",
        "if EVAL_CLAUDE_35SONNET:\n",
        "  judging_units.append(create_repeated_judge_layer(\"Claude 3.5\", \"claude-3-5-sonnet-20241022\"))\n",
        "  included_judges.append(\"Claude 3.5\")\n",
        "if EVAL_GEMINI_15PRO:\n",
        "  judging_units.append(create_repeated_judge_layer(\"Gemini 1.5 Pro\", \"gemini/gemini-1.5-pro\"))\n",
        "  included_judges.append(\"Gemini 1.5 Pro\")\n",
        "if EVAL_GROK_2:\n",
        "  judging_units.append(create_repeated_judge_layer(\"Grok 2\", \"xai/grok-2\"))\n",
        "  included_judges.append(\"Grok 2\")\n",
        "\n",
        "# =============================================================================\n",
        "# Assemble Verdict Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "pipeline = Pipeline(\"LLM Majority Judge\") >> Layer(judging_units)\n",
        "\n",
        "# =============================================================================\n",
        "# Running the Verdict Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "total_papers = len(predictions)\n",
        "for idx in tqdm(range(1, 3)):\n",
        "  paper_id = f\"paper_{idx:03}\"\n",
        "  print(f\"\\n========== Processing {paper_id} ({idx+1}/{total_papers}) ==========\")\n",
        "\n",
        "  try:\n",
        "    ground_truth_json = json.loads(ground_truths[idx])\n",
        "    prediction_json   = json.loads(predictions[idx])\n",
        "    ground_truth      = str(ground_truth_json['methodology'])\n",
        "    prediction        = str(prediction_json['contributions'])\n",
        "  except Exception as e:\n",
        "    print(f\"Error extracting JSON fields for {paper_id}: {e}\")\n",
        "    continue\n",
        "\n",
        "  print(f\"[{paper_id}] Extracted ground truth and prediction.\")\n",
        "\n",
        "  content = Schema.of(\n",
        "      ground_truth=ground_truth,\n",
        "      predicted=prediction\n",
        "  )\n",
        "\n",
        "  result = pipeline.run(content)\n",
        "\n",
        "  judge_dict, key_list = result\n",
        "\n",
        "  mode_scores = {i:\"\" for i in included_judges}       # {'GPT-4o': 1, 'o3-mini': 2}\n",
        "  mode_explanations = {i:\"\" for i in included_judges} # {'GPT-4o': 'explanatio...', 'o3-mini': 'explanation...'}\n",
        "\n",
        "  modes_count = 0\n",
        "  explanations_count = 1\n",
        "  for i in range(len(included_judges)):\n",
        "    mode_scores[included_judges[i]] = judge_dict[key_list[modes_count]]\n",
        "    mode_explanations[included_judges[i]] = judge_dict[key_list[explanations_count]]\n",
        "    # key_list structure:\n",
        "    # ['LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_choice', 'LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_explanation',\n",
        "    # ['LLM Majority Judge_root.block.layer[1].block.block.unit[Map MaxPool]_choice', 'LLM Majority Judge_root.block.layer[1].block.block.unit[Map MaxPool]_explanation',...]\n",
        "    modes_count += 2\n",
        "    explanations_count += 2\n",
        "\n",
        "  print(f\"[{paper_id}] Mode scores: {mode_scores}\")\n",
        "  print(f\"[{paper_id}] Mode explanations: {mode_explanations}\")\n",
        "\n",
        "  # -----------------------------------------------------------------------------\n",
        "  # Aggregation + Save to CSV\n",
        "  # -----------------------------------------------------------------------------\n",
        "\n",
        "  # Currently same as taking the average, but could weigh models more heavily with a similar structure\n",
        "  # avg = sum(x)/count(x)\n",
        "  # avg = 1/count(x) * sum(x)\n",
        "  # same thing\n",
        "\n",
        "  model_weight = 1 / len(included_judges)\n",
        "  aggregated_score = sum(mode_scores.values()) * model_weight\n",
        "\n",
        "  print(f\"[{paper_id}] Aggregated score: {aggregated_score}\")\n",
        "\n",
        "  row = {\"paper\": paper_id}\n",
        "  row.update(mode_scores)\n",
        "  row[\"aggregated\"] = aggregated_score\n",
        "\n",
        "  df = pd.DataFrame([row])\n",
        "  ordered_cols = [\"paper\"] + included_judges + [\"aggregated\"]\n",
        "  df = df[ordered_cols]\n",
        "\n",
        "  # make file for paper_idx\n",
        "  os.makedirs(os.path.join(results_dir, paper_id), exist_ok=True)\n",
        "\n",
        "  for i in included_judges:\n",
        "    # make txt file for each judge within paper_id dir\n",
        "    explanation_file = os.path.join(results_dir, paper_id, f\"{i}.txt\")\n",
        "    with open(explanation_file, \"a\") as f:\n",
        "      # save explanations\n",
        "      f.write(mode_explanations[i])\n",
        "\n",
        "  csv_dir = os.path.join(results_dir, \"aggregated_results.csv\")\n",
        "\n",
        "  df.to_csv(csv_dir, mode = 'a', header = not os.path.exists(csv_dir), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EblLKsJmfov1",
        "outputId": "29d30f92-7e37-4a6e-97b9-626def12339b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "The predicted methodology and the original methodology share some high-level stages such as image acquisition, preprocessing (including segmentation with techniques like thresholding), feature extraction, and the use of traditional machine learning classifiers (e.g., SVM, RF). However, there are substantial differences. In the original, image acquisition is performed in an uncontrolled setting with specific details (e.g., fixed distance, capturing four images per tomato for comprehensive analysis) and an emphasis on transfer learning using pre‑trained deep networks (MobileNetv2, Inceptionv3, ResNet50, AlexNet) to extract deep features. In contrast, the predicted method uses an inline system with images captured on a conveyor belt under more controlled conditions and relies on handcrafted features (size, shape, color, texture) rather than deep feature extraction. Additionally, the predicted approach introduces a separate defect detection module (employing anomaly detection or a dedicated CNN) which is not part of the original pipeline. Moreover, while both approaches evaluate performance using classification metrics, the predicted method does not mirror the extensive experimentation and tuning steps outlined in the original paper.\n",
            "\n",
            "Overall, while there are some common elements, the key portion of using transfer learning for feature extraction and the detailed image acquisition strategy in the original are missing or altered significantly in the predicted methodology. Therefore, the predicted methodology shows only some similarity with the original and misses several crucial methodological details.\n",
            "LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_choice\n",
            "LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_explanation\n",
            "['LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_choice', 'LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_explanation', 'LLM Majority Judge_root.block.layer[1].block.block.unit[Map MaxPool]_choice', 'LLM Majority Judge_root.block.layer[1].block.block.unit[Map MaxPool]_explanation', 'LLM Majority Judge_root.block.layer[2].block.block.unit[Map MaxPool]_choice', 'LLM Majority Judge_root.block.layer[2].block.block.unit[Map MaxPool]_explanation']\n"
          ]
        }
      ],
      "source": [
        "print(judge_dict[\"LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_choice\"])\n",
        "print(judge_dict[\"LLM Majority Judge_root.block.layer[0].block.block.unit[Map MaxPool]_explanation\"])\n",
        "print(key_list[0])\n",
        "print(key_list[1])\n",
        "print(key_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}